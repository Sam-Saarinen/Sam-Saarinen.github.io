You were just too curious, weren't you? Was it intended that you never read the final page? Why didn't we ever direct you to read it?

One of the goals for this discussion was to highlight a fundamental tension between the automation of choices by AI, and the trust that we place in the system. Like the tension in a kite string, these two factors must both pull each other in order to achieve sustained flight. In order for us to benefit the most from AI, it needs to be able to operate without human intervention most of the time. But that requires trust.

Let's talk about how each aspect of this discussion was formatted and highlight some more fundamental tensions.

<h2>The Food</h2>
The food you were given was determined by the clothes that you wore to the seminar. This might be upsetting to you. You might have really wanted one thing, but been given something else because of how you looked to the system. Does race determine what kind of food you like? Definitely not. Yet, if you do a statistical survey world-wide, it correlates highly with questions like "What is your favorite food?", or "How often do you eat rice?" One of the dangers of only looking at statistical accuracy is that not all wrong cases are made equal. Particularly when it comes to human rights, preferences, and experiences, it's important to make sure that decisions are made for the right reasons. This is part of the motivation for a field of research called <strong>explainable AI</strong>.

<h2>The Seating</h2>
The seat you were assigned was based on your initial reaction to AI - positive or negative, and for what reason. The mood of the text you read was also changed by that initial reaction. Present-day (2018) AI algorithms for news recommendation, search engine results, or book recommendations take into account what you have previously read. If that happens to be mostly left-leaning political coverage of international wars, these systems are currently most likely to recommend more left-leaning political coverage of international wars. The phenomenon of only being recommended articles that are more and more similar to what you've already seen is called the <strong>filter bubble</strong>. This is our second fundamental tension: between easy-to-express objectives and what we really need.

One of my ((S)am's) research projects is developing better recommendation systems that break the filter bubble by explicitly considering the diversity of coverage offered and changing the objective being maximized by that recommendation system.

<h2>The Seminar</h2>
This seminar wasn't actually generated by "AI". In fact, anyone who went through an intro computer science class could have created it, and we used almost no data. Lots of things are branded as "AI" right now, but under the hood, they're actually quite simple. Maybe we could have built a much more elaborate system and collected real data to make an actually good data-driven design, but we weren't willing to spend that much time, effort, and money. This is the third tension, between the cost of improving a choice and the value of making a better choice. You as an individual probably aren't willing to spend the time making this seminar better. But if everyone thinks that way, maybe nobody will.

<h2>The Tensions</h2>
In short, AI is poised to change:
<ul>
    <li>Our Economy - What is the value of human labor?</li>
    <li>Our Humanity - What defines us, if our choices are taken from us?</li>
    <li>Our Identity - What makes one human different from another?</li>
</ul>

There are 3 fundamental tensions:
<ol>
    <li>Between the automation of a system and trust in that system.</li>
    <li>Between easy-to-express objectives and what we really need.</li>
    <li>Between the cost of improving a choice and the value of making that better choice.</li>
</ol>
___
